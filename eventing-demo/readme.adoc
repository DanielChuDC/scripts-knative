= Knative Eventing

From basics to Kafka using the direct https://docs.google.com/presentation/d/1kQn4HBUmSDH_EcRNPB8hAsoJetrVGM9hkDI0rriFGKA/edit#slide=id.g6234e6907d_0_40[Source to (Sink) Service] approach.  This means the return value of the Service (Sink) is ignored.

There are 3 "styles" of Knative Eventing, rising in complexity and in capability:

* Source to Service
  Single sink, event receiving service
  Simplest getting started experience
  No queuing, no backpressure, no filtering
  No replies

* Channels and Subscriptions
  Multiple sinks, mutiple event receiving services
  Various channel backends: In-memory, Kafka, GCP PubSub
  Supports replies
  No filtering

* Brokers and Triggers
  Multiple sinks, mutiple event receiving services
  Allows for filtering


== Sources/Producers, Sinks, Services, Consumers/Receivers

TODO
https://github.com/knative/docs/tree/master/docs/eventing/sources

Diagrams

== Prerequistes:

Create an env variable called BOOK_HOME to point to where you git cloned 

git clone -b knative-cookbook https://github.com/redhat-developer-demos/knative-tutorial

export BOOK_HOME=~/11steps/knative-tutorial/

* Working Minikube
----
./1_startMinikube.sh
----

* Istio Installed
----
./2_install_istio.sh
----

* Knative CRDS Installed

----
./3_install_knative_crds.sh
----

* Knative Serving Installed

----
./4_install_knative_serving.sh
----

* Knative Eventing Installed

----
./5_install_knative_eventing.sh
----

=== Check Knative CRDS
----
kubectl get crds | grep knative
apiserversources.sources.eventing.knative.dev        2019-12-28T22:48:39Z
brokers.eventing.knative.dev                         2019-12-28T22:48:39Z
certificates.networking.internal.knative.dev         2019-12-28T22:48:38Z
channels.messaging.knative.dev                       2019-12-28T22:48:39Z
configurations.serving.knative.dev                   2019-12-28T22:48:38Z
containersources.sources.eventing.knative.dev        2019-12-28T22:48:39Z
cronjobsources.sources.eventing.knative.dev          2019-12-28T22:48:39Z
eventtypes.eventing.knative.dev                      2019-12-28T22:48:39Z
images.caching.internal.knative.dev                  2019-12-28T22:48:38Z
ingresses.networking.internal.knative.dev            2019-12-28T22:48:38Z
inmemorychannels.messaging.knative.dev               2019-12-28T22:48:39Z
metrics.autoscaling.internal.knative.dev             2019-12-28T22:48:38Z
parallels.flows.knative.dev                          2019-12-28T22:48:39Z
parallels.messaging.knative.dev                      2019-12-28T22:48:39Z
podautoscalers.autoscaling.internal.knative.dev      2019-12-28T22:48:38Z
revisions.serving.knative.dev                        2019-12-28T22:48:38Z
routes.serving.knative.dev                           2019-12-28T22:48:38Z
sequences.flows.knative.dev                          2019-12-28T22:48:39Z
sequences.messaging.knative.dev                      2019-12-28T22:48:39Z
serverlessservices.networking.internal.knative.dev   2019-12-28T22:48:38Z
services.serving.knative.dev                         2019-12-28T22:48:38Z
sinkbindings.sources.eventing.knative.dev            2019-12-28T22:48:39Z
subscriptions.messaging.knative.dev                  2019-12-28T22:48:39Z
triggers.eventing.knative.dev                        2019-12-28T22:48:39Z
----

=== Check Knative Serving Installation
----
kubectl get pods -n knative-serving
NAME                                READY   STATUS    RESTARTS   AGE
activator-764c84b867-hmdsq          1/1     Running   0          6h34m
autoscaler-6d78bfc6f-8v9d8          1/1     Running   0          6h34m
autoscaler-hpa-595fcbc958-r77tm     1/1     Running   0          6h34m
controller-7f9d9ffff7-9gws9         1/1     Running   0          6h34m
networking-istio-65cb5dc74d-dmfsh   1/1     Running   0          6h34m
webhook-8fb4f7f8-fzfvx              1/1     Running   0          6h34m
----

=== Check Knative Eventing Installation
----
kubectl get pods -n knative-eventing
NAME                                   READY   STATUS    RESTARTS   AGE
eventing-controller-666b79d867-kq8cc   1/1     Running   0          6h34m
eventing-webhook-5867c98d9b-hzctw      1/1     Running   0          6h34m
imc-controller-7c4f9945d7-s59xd        1/1     Running   0          6h34m
imc-dispatcher-7b55b86649-nsjm2        1/1     Running   0          6h34m
sources-controller-694f8df9c4-pss2w    1/1     Running   0          6h34m
----

=== Check CRDs & api-resources
----
kubectl get crds | grep knative
apiserversources.sources.eventing.knative.dev        2019-12-31T14:05:46Z
brokers.eventing.knative.dev                         2019-12-31T14:05:46Z
certificates.networking.internal.knative.dev         2019-12-31T14:05:45Z
channels.messaging.knative.dev                       2019-12-31T14:05:46Z
configurations.serving.knative.dev                   2019-12-31T14:05:45Z
containersources.sources.eventing.knative.dev        2019-12-31T14:05:46Z
cronjobsources.sources.eventing.knative.dev          2019-12-31T14:05:46Z
eventtypes.eventing.knative.dev                      2019-12-31T14:05:47Z
images.caching.internal.knative.dev                  2019-12-31T14:05:45Z
ingresses.networking.internal.knative.dev            2019-12-31T14:05:45Z
inmemorychannels.messaging.knative.dev               2019-12-31T14:05:47Z
metrics.autoscaling.internal.knative.dev             2019-12-31T14:05:45Z
parallels.flows.knative.dev                          2019-12-31T14:05:47Z
parallels.messaging.knative.dev                      2019-12-31T14:05:47Z
podautoscalers.autoscaling.internal.knative.dev      2019-12-31T14:05:45Z
revisions.serving.knative.dev                        2019-12-31T14:05:45Z
routes.serving.knative.dev                           2019-12-31T14:05:45Z
sequences.flows.knative.dev                          2019-12-31T14:05:47Z
sequences.messaging.knative.dev                      2019-12-31T14:05:47Z
serverlessservices.networking.internal.knative.dev   2019-12-31T14:05:45Z
services.serving.knative.dev                         2019-12-31T14:05:45Z
sinkbindings.sources.eventing.knative.dev            2019-12-31T14:05:47Z
subscriptions.messaging.knative.dev                  2019-12-31T14:05:47Z
triggers.eventing.knative.dev                        2019-12-31T14:05:47Z
----

----
kubectl api-resources --api-group='serving.knative.dev'
NAME             SHORTNAMES      APIGROUP              NAMESPACED   KIND
configurations   config,cfg      serving.knative.dev   true         Configuration
revisions        rev             serving.knative.dev   true         Revision
routes           rt              serving.knative.dev   true         Route
services         kservice,ksvc   serving.knative.dev   true         Service
----

----
kubectl api-resources --api-group='messaging.knative.dev'
NAME               SHORTNAMES   APIGROUP                NAMESPACED   KIND
channels           ch           messaging.knative.dev   true         Channel
inmemorychannels   imc          messaging.knative.dev   true         InMemoryChannel
parallels                       messaging.knative.dev   true         Parallel
sequences                       messaging.knative.dev   true         Sequence
subscriptions      sub          messaging.knative.dev   true         Subscription
----

----
kubectl api-resources --api-group='eventing.knative.dev'
NAME         SHORTNAMES   APIGROUP               NAMESPACED   KIND
brokers                   eventing.knative.dev   true         Broker
eventtypes                eventing.knative.dev   true         EventType
triggers                  eventing.knative.dev   true         Trigger
----

----
kubectl api-resources --api-group='sources.eventing.knative.dev'
NAME               SHORTNAMES   APIGROUP                       NAMESPACED   KIND
apiserversources                sources.eventing.knative.dev   true         ApiServerSource
containersources                sources.eventing.knative.dev   true         ContainerSource
cronjobsources                  sources.eventing.knative.dev   true         CronJobSource
sinkbindings                    sources.eventing.knative.dev   true         SinkBinding
----

== Simple Knative Eventing

=== Deploy Knative Serving Service

Knative Eventing targets a Kubernetes Service, in our case, we will use a Knative Serving Service as the target of the event - the Sink.

There are 3 options for Knative Serving Services to choose from:

* qeventinghello - Quarkus with JAX-RS API, offering both JVM and native 
* sbeventingce - Spring Boot with RestController API
* sbeventinghello - Spring Boot with Servlet API

The primary focus on the code is to receive the POST and to output the CloudEvent relevant information:

----
System.out.println("ce-id=" + http.getHeaders().get("ce-id"));
System.out.println("ce-source=" + http.getHeaders().get("ce-source"));
System.out.println("ce-specversion=" + http.getHeaders().get("ce-specversion"));
System.out.println("ce-time=" + http.getHeaders().get("ce-time"));
System.out.println("ce-type=" + http.getHeaders().get("ce-type"));
System.out.println("content-type=" + http.getHeaders().getContentType());
System.out.println("content-length=" + http.getHeaders().getContentLength());

System.out.println("POST:" + http.getBody());
----

CloudEvent to HTTP mapping information can be found here
https://github.com/cloudevents/spec/blob/master/http-protocol-binding.md#3-http-message-mapping

=== Create Namespace

Deploy the Knative Serving Service into namespace myeventing:

----
kubectl create namespace myeventing
kubectl config set-context --current --namespace=myeventing
----

To deploy the event consumer/receiver, the sink service
----
cat <<EOF | kubectl apply -f -
apiVersion: serving.knative.dev/v1alpha1
kind: Service
metadata:
  name: eventinghello
spec:
  template:
    metadata:
      name: eventinghello-v1
      annotations:
        # disable istio-proxy injection
        sidecar.istio.io/inject: "false"
        autoscaling.knative.dev/target: "1"
    spec:
      containers:
      - image: quay.io/burrsutter/eventinghello:0.0.1
EOF
----

or you can build its .jar and its container image:

----
cd sbeventingce
mvn clean compile package
eval $(minikube docker-env)
./dockerbuild.sh
cd ..
----

Verify that eventinghello deployed

----
kubectl get ksvc
NAME            URL                                           LATESTCREATED      LATESTREADY        READY   REASON
eventinghello   http://eventinghello.myeventing.example.com   eventinghello-v1   eventinghello-v1   True
----

The default behavior of Knative Serving is that the very first deployment of a Knative Serving Service will automatically scale up to 1 and after about 90 seconds it will auto-scale down to zero.

You can watch the pod lifecycle with the following command:

----
watch kubectl get pods
----

Let eventinghello scale to zero pods before moving on.

=== Deploy CronJobSource
----
cat <<EOF | kubectl apply -f -
apiVersion: sources.eventing.knative.dev/v1alpha1
kind: CronJobSource
metadata:
  name: eventinghello-cronjob-source
spec:
  schedule: "*/2 * * * *"
  data: '{"key": "every 2 mins"}'
  sink:
    apiVersion: serving.knative.dev/v1alpha1
    kind: Service
    name: eventinghello
EOF
----

----
kubectl get cronjobsource
NAME                           READY   AGE
eventinghello-cronjob-source   True    10s
----

This produces a pod with a prefix of "cronjobsource-eventinghell"

----
watch kubectl get pods
NAME                                            READY   STATUS    RESTARTS   AGE
cronjobsource-eventinghell-54b9ef12-2c2f-11ea   1/1     Running   0          14s
----

After approximately 2 minutes, it will cause the eventinghello pod to scale up 

----
watch kubectl get pods
NAME                                           READY   STATUS    RESTARTS   AGE
cronjobsource-eventinghell-54b9ef12-2c2f-11ea  1/1     Running   0          97s
eventinghello-v1-deployment-7cfcb664ff-r694p   2/2     Running   0          10s
----

And after approximately 60 seconds, eventinghello will auto-scale down to zero pods.

----
watch kubectl get pods
NAME                                           READY   STATUS        RESTARTS   AGE
cronjobsource-eventinghell-54b9ef12-2c2f-11ea  1/1     Running       0          2m28s
eventinghello-v1-deployment-7cfcb664ff-r694p   2/2     Terminating   0          65s
----

You can follow logs to see the CloudEvent details with the following command:
----
stern eventinghello -c user-container

ce-id=a1e0cbea-8f66-4fa6-8f3c-e5590c4ee147
ce-source=/apis/v1/namespaces/myeventing/cronjobsources/eventinghello-cronjob-source
ce-specversion=1.0
ce-time=2020-01-01T00:44:00.000889221Z
ce-type=dev.knative.cronjob.event
content-type=application/json
content-length=22
POST:{"key":"every 2 mins"}
----

=== Clean Up
----
kubectl delete namespace myeventing
----

== Kafka+Knative Eventing

In this section, we will deploy Kafka (via Strimzi), the Knative Kafka Source and have the messages flowing through the Kafka topic as the event that causes the scale-up of the sink Knative Serving Service called eventhinghello.

=== Deploy Kafka for Kubernetes (Strimzi) inside Minikube

https://strimzi.io/quickstarts/minikube/

----

kubectl create namespace kafka
kubectl config set-context --current --namespace=kafka

curl -L https://github.com/strimzi/strimzi-kafka-operator/releases/download/0.15.0/strimzi-cluster-operator-0.15.0.yaml \
  | sed 's/namespace: .*/namespace: kafka/' \
  | kubectl apply -f - -n kafka 

----

The result will be the single strimzi-cluster-operator
----
kubectl get pods
strimzi-cluster-operator-85f596bfc7-7dgds     1/1     Running   0          1m2s
----

=== Deploy a Kafka Cluster inside Minikube 

----
kubectl apply -f https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/0.15.0/examples/kafka/kafka-persistent-single.yaml -n kafka 
----

The result will be a single Zookeeper, single Kafka broker and the entity-operator

----
kubectl get pods 
NAME                                          READY   STATUS    RESTARTS   AGE
my-cluster-entity-operator-7d677bdf7b-jpws7   3/3     Running   0          85s
my-cluster-kafka-0                            2/2     Running   0          110s
my-cluster-zookeeper-0                        2/2     Running   0          2m22s
strimzi-cluster-operator-85f596bfc7-7dgds     1/1     Running   0          4m22s
----

=== Create Kafka Topic my-topic

----
cat <<EOF | kubectl apply -f -
apiVersion: kafka.strimzi.io/v1alpha1
kind: KafkaTopic
metadata:
  name: my-topic
  labels:
    strimzi.io/cluster: my-cluster
spec:
  partitions: 10
  replicas: 1
EOF
----

Check that the topic was created
----
kubectl get kafkatopics
NAME       PARTITIONS   REPLICATION FACTOR
my-topic   10           1
----

Create some test messages

Terminal 1 - Producer
----
$BOOK_HOME/bin/kafka-producer.sh
>one
>two
>three
----

Terminal 2 - Consumer
----
$BOOK_HOME/bin/kafka-consumer.sh
one
two
three
----


Ctrl-C to stop producer & consumer


=== Create the Knative Kafka Source Infrastructure

Create the Knative Kafka Source

----
kubectl apply -f https://github.com/knative/eventing-contrib/releases/download/v0.11.0/kafka-source.yaml
----

This step creates Knative Kafka Source in the knative-sources namespace as well as a CRD, ServiceAccount, ClusterRole, etc 

----
kubectl get pods -n knative-sources
NAME                         READY   STATUS    RESTARTS   AGE
kafka-controller-manager-0   1/1     Running   0          1m17s
----

Create the Knative Kafka Channel

----
curl -L "https://github.com/knative/eventing-contrib/releases/download/v0.11.0/kafka-channel.yaml" \
 | sed 's/REPLACE_WITH_CLUSTER_URL/my-cluster-kafka-bootstrap.kafka:9092/' \
 | kubectl apply --filename -  
----

note: "my-cluster-kafka-bootstrap.kafka:9092" comes from "kubectl get services -n kafka"

Look for 3 new pods in namespace knative-eventing with the prefix "kafka"

----
kubectl get pods -n knative-eventing
NAME                                   READY   STATUS    RESTARTS   AGE
eventing-controller-666b79d867-kq8cc   1/1     Running   0          64m
eventing-webhook-5867c98d9b-hzctw      1/1     Running   0          64m
imc-controller-7c4f9945d7-s59xd        1/1     Running   0          64m
imc-dispatcher-7b55b86649-nsjm2        1/1     Running   0          64m
kafka-ch-controller-7c596b6b55-fzxcx   1/1     Running   0          33s
kafka-ch-dispatcher-577958f994-4f2qs   1/1     Running   0          33s
kafka-webhook-74bbd99f5c-c84ls         1/1     Running   0          33s
sources-controller-694f8df9c4-pss2w    1/1     Running   0          64m  
----

and some new CRDs

----
kubectl get crds | grep kafkasources
kafkasources.sources.eventing.knative.dev            2019-12-28T14:53:14Z

kubectl get crds | grep kafkachannels
kafkachannels.messaging.knative.dev                  2019-12-28T15:00:22Z
----

=== Deploy Knative Serving Sink Service

Deploy the Knative Serving Service - the event consumer, the sink

----
cat <<EOF | kubectl apply -f -
apiVersion: serving.knative.dev/v1alpha1
kind: Service
metadata:
  name: eventinghello
spec:
  template:
    metadata:
      name: eventinghello-v1
      annotations:
        # disable istio-proxy injection
        sidecar.istio.io/inject: "false"
        autoscaling.knative.dev/target: "1"
    spec:
      containers:
      - image: quay.io/burrsutter/eventinghello:0.0.1
EOF
----

----
kubectl get ksvc
NAME            URL                                      LATESTCREATED      LATESTREADY        READY   REASON
eventinghello   http://eventinghello.kafka.example.com   eventinghello-v1   eventinghello-v1   True
----

Follow the logs

----
stern eventinghello -c user-container
----

"eventinghello" will be around until it hits its scale-down time limit.

=== Create KafkaSource for my-topic

----
cat <<EOF | kubectl apply -f -
apiVersion: sources.eventing.knative.dev/v1alpha1
kind: KafkaSource
metadata:
  name: mykafka-source
spec:
  consumerGroup: knative-group
  bootstrapServers: my-cluster-kafka-bootstrap:9092 
  topics: my-topic
  sink:
    apiVersion: serving.knative.dev/v1alpha1
    kind: Service
    name: eventinghello
EOF
----

This will result in a new pod prefixed with "mykafka-source". 

----
kubectl get pods
NAME                                           READY   STATUS    RESTARTS   AGE
my-cluster-entity-operator-7d677bdf7b-jpws7    3/3     Running   0          22m
my-cluster-kafka-0                             2/2     Running   0          22m
my-cluster-zookeeper-0                         2/2     Running   0          23m
mykafka-source-vxs2k-56548756cc-j7m7v          1/1     Running   0          11s
strimzi-cluster-operator-85f596bfc7-7dgds      1/1     Running   0          25m
----

Note: since we had some test messages of "one", "two" and "three" from earlier you should see the eventinghello service awaken to process those messages.  Since the "one", "two", and "three" were not entered as JSON, your http body will not be interpreted correctly.  Knative Eventing endpoints need JSON input.

Wait for eventinghello to scale down before moving on.

=== Publish some messages

Note: Knative Eventing messages needs to be JSON formatted

----
$BOOK_HOME/bin/kafka-producer.sh
----

And provide the following JSON-formatted messages

----
{"hello":"world"}

{"hola":"mundo"}

{"bonjour":"le monde"}

{"hey": "duniya"}
----

While monitoring the logs 
----
stern eventinghello -c user-container
----

Output shortened for brevity and formatting reasons
----
ce-id=partition:1/offset:1
ce-source=/apis/v1/namespaces/kafka/kafkasources/mykafka-source#my-topic
ce-specversion=1.0
ce-time=2020-01-01T01:16:12.886Z
ce-type=dev.knative.kafka.event
content-type=application/json
content-length=17
POST:{"hey": "duniya"}
----


Ctrl-C to terminate producer

=== Produce a bunch of messages

The Knative Serving Sink Service was defined with the following annotation

----
autoscaling.knative.dev/target: "1"
----

This means a concurrency factor of one, if you are able to push in a lot of Kafka message rapidly, you will see more than one eventinghello pod scaled up to handle the load.

Now you just need to hit the right endpoint on the Kafka Spammer application to push in 3 messages.

----
kubectl -n kafka run kafka-spammer --image=quay.io/burrsutter/kafkaspammer:1.0.2 

kubectl exec -i -t $(kubectl get pod -l "run=kafka-spammer" -o jsonpath='{.items[0].metadata.name}') -- /bin/sh

curl localhost:8080/3
----


You should see about 3 eventinghello pods springing to life
----
kubectl get pods  
NAME                                           READY   STATUS    RESTARTS   AGE
eventinghello-v1-deployment-65c9b9c7df-8rwqc   1/2     Running   0          6s
eventinghello-v1-deployment-65c9b9c7df-q7pcf   1/2     Running   0          4s
eventinghello-v1-deployment-65c9b9c7df-zht2t   1/2     Running   0          6s
kafka-spammer-77ccd4f9c6-sx5j4                 1/1     Running   0          26s
my-cluster-entity-operator-7d677bdf7b-jpws7    3/3     Running   0          27m
my-cluster-kafka-0                             2/2     Running   0          27m
my-cluster-zookeeper-0                         2/2     Running   0          28m
mykafka-source-vxs2k-56548756cc-j7m7v          1/1     Running   0          5m12s
strimzi-cluster-operator-85f596bfc7-7dgds      1/1     Running   0          30m
----

After approximately 60 seconds and no new messages showing up in my-topic, the 3 eventinghello pods will terminate, scale-down to zero

Note: these messages are NOT being evenly distributed across the various eventinghello pods, the first one up starts consuming them immediately.

image::sending_3.png[Sending 3 messages]

To close out the spammer and remove it 
----
exit 
kubectl delete deployment kafka-spammer
----

=== Clean up

----
kubectl delete namespace kafka
----


== Knative Eventing: Channels & Subscriptions

If you want more than one Sink use Channels and Subscriptions to decouple the producers & consumers of events

=== Channels

5 Step Process

1) Create a Namespace

2) Create Channel

3) Create Source to Channel

4) Create Sink Service

5) Create Subscription of Channel to Sink Service

=== 1. Create Namespace
----
kubectl create namespace myeventing
kubectl config set-context --current --namespace=myeventing
----

=== 2. Create Channel

----
cat <<EOF | kubectl apply -f -
apiVersion: messaging.knative.dev/v1alpha1
kind: InMemoryChannel
metadata:
  name: eventinghello-ch
EOF
----

cat <<EOF | kubectl apply -f -
apiVersion: messaging.knative.dev/v1alpha1
kind: Channel
metadata:
  name: eventinghello-ch
EOF

----
kubectl get inmemorychannel
kubectl describe inmemorychannel eventinghello-ch
----

=== 3. Create Source that will send to the Channel

----
cat <<EOF | kubectl apply -f -
apiVersion: sources.eventing.knative.dev/v1alpha1
kind: CronJobSource
metadata:
  name: my-cjs
spec:
  schedule: "*/2 * * * *"
  data: '{"message": "From CronJob Source"}'
  sink:
    apiVersion: messaging.knative.dev/v1alpha1
    kind: InMemoryChannel
    name: eventinghello-ch
EOF
----

cat <<EOF | kubectl apply -f -
apiVersion: sources.eventing.knative.dev/v1alpha1
kind: CronJobSource
metadata:
  name: my-cjs
spec:
  schedule: "*/2 * * * *"
  data: '{"message": "From CronJob Source"}'
  sink:
    apiVersion: messaging.knative.dev/v1alpha1
    kind: Channel
    name: eventinghello-ch
EOF


----
kubectl get cronjobsource
kubectl describe cronjobsource my-cjs
----


=== 4. Create Sink Services

----
cat <<EOF | kubectl apply -f -
apiVersion: serving.knative.dev/v1alpha1
kind: Service
metadata:
  name: eventinghelloa
spec:
  template:
    metadata:
      name: eventinghelloa-1
      annotations:
        # disable istio-proxy injection
        sidecar.istio.io/inject: "false"
        autoscaling.knative.dev/target: "1"
    spec:
      containers:
      - image: quay.io/burrsutter/eventinghello:0.0.1
EOF
----

----
cat <<EOF | kubectl apply -f -
apiVersion: serving.knative.dev/v1alpha1
kind: Service
metadata:
  name: eventinghellob
spec:
  template:
    metadata:
      name: eventinghellob-1
      annotations:
        # disable istio-proxy injection
        sidecar.istio.io/inject: "false"
        autoscaling.knative.dev/target: "1"
    spec:
      containers:
      - image: quay.io/burrsutter/eventinghello:0.0.1
EOF
----

=== 5. Create Subscriptions to Channel 

----
cat <<EOF | kubectl apply -f -
apiVersion: messaging.knative.dev/v1alpha1
kind: Subscription
metadata:
  name: eventinghelloa-sub
spec:
  channel:
    apiVersion: messaging.knative.dev/v1alpha1
    kind: InMemoryChannel
    name: eventinghello-ch
  subscriber:
    ref:
      apiVersion: serving.knative.dev/v1alpha1
      kind: Service
      name: eventinghelloa
EOF
----

----
cat <<EOF | kubectl apply -f -
apiVersion: messaging.knative.dev/v1alpha1
kind: Subscription
metadata:
  name: eventinghellob-sub
spec:
  channel:
    apiVersion: messaging.knative.dev/v1alpha1
    kind: InMemoryChannel
    name: eventinghello-ch
  subscriber:
    ref:
      apiVersion: serving.knative.dev/v1alpha1
      kind: Service
      name: eventinghellob
EOF
----

Wait the 2 minutes for the cronjobsource and see eventinghelloa and b start to run

----
kubectl get pods
NAME                                                              READY   STATUS    RESTARTS   AGE
cronjobsource-my-cjs-93544f14-2bf9-11ea-83c7-08002737670c-6br6x   1/1     Running   0          2m15s
eventinghelloa-1-deployment-d86bf4847-hvbk6                       2/2     Running   0          5s
eventinghellob-1-deployment-5c986c7586-4clpb                      2/2     Running   0          5s
----

Wait approximately 60 seconds for eventinghelloa and b to auto-scaled down to zero

----
kubectl get pods
NAME                                                              READY   STATUS        RESTARTS   AGE
cronjobsource-my-cjs-93544f14-2bf9-11ea-83c7-08002737670c-6br6x   1/1     Running       0          7m15s
eventinghelloa-1-deployment-d86bf4847-hvbk6                       2/2     Terminating   0          65s
eventinghellob-1-deployment-5c986c7586-4clpb                      2/2     Terminating   0          65s
----

and if you wait about 60 more seconds, you will see the dev.knative.cronjob.event cause the auto-scale up of eventinghelloa and b.

=== Clean up

----
kubectl delete namespace myeventing
----

== Brokers & Triggers

1. Create the namespace and inject the broker into your namespace
2. Create the consumers/receivers
3. Create triggers
4. Push some messages

=== 1. Create the namespace and inject the broker into your namespace

----
kubectl create namespace myeventing
kubectl config set-context --current --namespace=myeventing
kubectl label namespace myeventing knative-eventing-injection=enabled
----

----
kubectl --namespace myeventing get Broker 
----

----
kubectl get pods
NAME                                           READY   STATUS        RESTARTS   AGE
default-broker-filter-c6654bccf-qb272          1/1     Running       0          62s
default-broker-ingress-7479966dc7-99xvm        1/1     Running       0          62s
----

=== 2. Create the consumers/receivers

----
cat <<EOF | kubectl apply -f -
apiVersion: serving.knative.dev/v1alpha1
kind: Service
metadata:
  name: eventingbonjour
spec:
  template:
    metadata:
      name: eventingbonjour-1
      annotations:
        # disable istio-proxy injection
        sidecar.istio.io/inject: "false"
        autoscaling.knative.dev/target: "1"
    spec:
      containers:
      - image: quay.io/burrsutter/eventinghello:0.0.1
EOF
----

----
cat <<EOF | kubectl apply -f -
apiVersion: serving.knative.dev/v1alpha1
kind: Service
metadata:
  name: eventingaloha
spec:
  template:
    metadata:
      name: eventingaloha-1
      annotations:
        # disable istio-proxy injection
        sidecar.istio.io/inject: "false"
        autoscaling.knative.dev/target: "1"
    spec:
      containers:
      - image: quay.io/burrsutter/eventinghello:0.0.1
EOF
----

----
kubectl get ksvc
NAME              URL                                             LATESTCREATED       LATESTREADY         READY   REASON
eventingaloha     http://eventingaloha.myeventing.example.com     eventingaloha-1     eventingaloha-1     True
eventingbonjour   http://eventingbonjour.myeventing.example.com   eventingbonjour-1   eventingbonjour-1   True
----

=== 3. Create triggers
----
kubectl apply --filename - << EOF
apiVersion: eventing.knative.dev/v1alpha1
kind: Trigger
metadata:
  name: hellobonjour
spec:
  filter:
    attributes:
      type: greeting
  subscriber:
    ref:
     apiVersion: serving.knative.dev/v1alpha1
     kind: Service
     name: eventingbonjour
EOF
----

----
kubectl apply --filename - << EOF
apiVersion: eventing.knative.dev/v1alpha1
kind: Trigger
metadata:
  name: helloaloha
spec:
  filter:
    attributes:
      type: greeting
  subscriber:
    ref:
     apiVersion: serving.knative.dev/v1alpha1
     kind: Service
     name: eventingaloha
EOF
----

----
kubectl get triggers
NAME           READY   REASON   BROKER    SUBSCRIBER_URI                                        AGE
helloaloha     True             default   http://eventingaloha.myeventing.svc.cluster.local     24s
hellobonjour   True             default   http://eventingbonjour.myeventing.svc.cluster.local   48s
----

Get their subscriberURIs
----
kubectl get trigger hellobonjour -o jsonpath='{.status.subscriberURI}'
http://eventingbonjour.myeventing.svc.cluster.local
----

----
kubectl get trigger helloaloha -o jsonpath='{.status.subscriberURI}'
http://eventingaloha.myeventing.svc.cluster.local
----

You will need those URIs in a below

=== 4. Push some messages

Start stream the logs for the event consumers

----
stern eventing -c user-container
----

Create a pod for using Curl
----
kubectl apply --filename - << EOF
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: curler
  name: curler
spec:
  containers: 
  - name: curler 
    image: fedora:29
    tty: true
EOF
----

Exec into the pod
----
kubectl exec -i -t $(kubectl get pod -l "run=curler" -o jsonpath='{.items[0].metadata.name}') -- /bin/bash
----

Curl the subcriberURI for eventingbonjour

----
curl -v "http://eventingbonjour.myeventing.svc.cluster.local" \
-X POST \
-H "Ce-Id: say-hello" \
-H "Ce-Specversion: 0.3" \
-H "Ce-Type: bonjour" \
-H "Ce-Source: mycurl" \
-H "Content-Type: application/json" \
-d '{"key":"from a curl"}'
----

Curl the subcriberURI for eventingaloha

----
curl -v "http://eventingaloha.myeventing.svc.cluster.local" \
-X POST \
-H "Ce-Id: say-hello" \
-H "Ce-Specversion: 0.3" \
-H "Ce-Type: aloha" \
-H "Ce-Source: mycurl" \
-H "Content-Type: application/json" \
-d '{"key":"from a curl"}'
----

Curl the subcriberURI for broker

----
kubectl get broker default -o jsonpath='{.status.address.url}'
http://default-broker.myeventing.svc.cluster.local
----

Note: Ce-Type: greeting

----
curl -v "http://default-broker.myeventing.svc.cluster.local" \
-X POST \
-H "Ce-Id: say-hello" \
-H "Ce-Specversion: 0.3" \
-H "Ce-Type: greeting" \
-H "Ce-Source: mycurl" \
-H "Content-Type: application/json" \
-d '{"key":"from a curl"}'
----

Both eventingbonjour and eventingaloha will come to life
----
eventingaloha-1-deployment-6fd4689467-xfsgv     2/2     Running       0          5s
eventingbonjour-1-deployment-677bbf6598-8mfd9   2/2     Running       0          5s
----

=== Clean up

----
kubectl delete namespace myeventing
----


=== Default Channel

In addition to using Kafka as an event Source, you can replace the default in-memory channel of Knative Eventing with a Kafka broker as well.

First inspect the default configuration for Knative Eventing

----
kubectl get cm default-ch-webhook -n knative-eventing -o yaml --export

apiVersion: v1
data:
  default-ch-config: |
    clusterDefault:
      apiVersion: messaging.knative.dev/v1alpha1
      kind: InMemoryChannel
    namespaceDefaults:
      some-namespace:
        apiVersion: messaging.knative.dev/v1alpha1
        kind: InMemoryChannel
kind: ConfigMap
metadata:
  annotations:
  creationTimestamp: null
  name: default-ch-webhook
  selfLink: /api/v1/namespaces/knative-eventing/configmaps/default-ch-webhook
----

Now, apply a new configuration that adds the "kafka" namespace (or whichever namespace you are working with)

----
cat <<-EOF | kubectl apply -f -
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: default-ch-webhook
  namespace: knative-eventing
data:
  default-ch-config: |
    clusterDefault:
      apiVersion: messaging.knative.dev/v1alpha1
      kind: InMemoryChannel
    namespaceDefaults:
      kafka:
        apiVersion: messaging.knative.dev/v1alpha1
        kind: KafkaChannel
        spec:
          numPartitions: 1
          replicationFactor: 1
EOF
----

kubectl delete pod -l app=webhook -n knative-serving

kubectl exec -it -n kafka -c kafka my-cluster-kafka-0 /bin/bash

cd bin

ls

./kafka-topics.sh --zookeeper localhost:2181 --list

./kafka-topics.sh --zookeeper localhost:2181 --describe --topic my-topic

== Extra Stuff

Build & Deploy the simple Kafka Spammer application to push messages faster

----
cd kafkaspammer
./1_jvmbuild.sh
eval $(minikube docker-env)
./2_dockerbuild_jvm.sh
./3_deploy.sh
----

