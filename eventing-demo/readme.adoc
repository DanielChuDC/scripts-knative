= Knative Eventing

From basics to Kafka using the direct https://docs.google.com/presentation/d/1kQn4HBUmSDH_EcRNPB8hAsoJetrVGM9hkDI0rriFGKA/edit#slide=id.g6234e6907d_0_40[Source to (Sink) Service] approach.  This means the return value of the Service (Sink) is ignored.

There are 3 "styles" of Knative Eventing, rising in complexity and in capability:

* Source to Service
  Single sink, event receiving service
  Simplest getting started experience
  No queuing, no backpressure, no filtering
  No replies

* Channels and Subscriptions
  Multiple sinks, mutiple event receiving services
  Various channel backends: In-memory, Kafka, GCP PubSub
  Supports replies
  No filtering

* Brokers and Triggers
  Multiple sinks, mutiple event receiving services
  Allows for filtering

== Sources & Sinks


== Simple Knative Eventing

Prerequistes include:

* Working Minikube
----
./1_startMinikube.sh
----

* Istio Installed
----
./2_install_istio.sh
----

* Knative CRDS Installed

----
./3_install_knative_crds.sh
----

* Knative Serving Installed

----
./4_install_knative_serving.sh
----

* Knative Eventing Installed

----
./5_install_knative_eventing.sh
----

=== Check Knative CRDS
----
kubectl get crds | grep knative
apiserversources.sources.eventing.knative.dev        2019-12-28T22:48:39Z
brokers.eventing.knative.dev                         2019-12-28T22:48:39Z
certificates.networking.internal.knative.dev         2019-12-28T22:48:38Z
channels.messaging.knative.dev                       2019-12-28T22:48:39Z
configurations.serving.knative.dev                   2019-12-28T22:48:38Z
containersources.sources.eventing.knative.dev        2019-12-28T22:48:39Z
cronjobsources.sources.eventing.knative.dev          2019-12-28T22:48:39Z
eventtypes.eventing.knative.dev                      2019-12-28T22:48:39Z
images.caching.internal.knative.dev                  2019-12-28T22:48:38Z
ingresses.networking.internal.knative.dev            2019-12-28T22:48:38Z
inmemorychannels.messaging.knative.dev               2019-12-28T22:48:39Z
metrics.autoscaling.internal.knative.dev             2019-12-28T22:48:38Z
parallels.flows.knative.dev                          2019-12-28T22:48:39Z
parallels.messaging.knative.dev                      2019-12-28T22:48:39Z
podautoscalers.autoscaling.internal.knative.dev      2019-12-28T22:48:38Z
revisions.serving.knative.dev                        2019-12-28T22:48:38Z
routes.serving.knative.dev                           2019-12-28T22:48:38Z
sequences.flows.knative.dev                          2019-12-28T22:48:39Z
sequences.messaging.knative.dev                      2019-12-28T22:48:39Z
serverlessservices.networking.internal.knative.dev   2019-12-28T22:48:38Z
services.serving.knative.dev                         2019-12-28T22:48:38Z
sinkbindings.sources.eventing.knative.dev            2019-12-28T22:48:39Z
subscriptions.messaging.knative.dev                  2019-12-28T22:48:39Z
triggers.eventing.knative.dev                        2019-12-28T22:48:39Z
----

=== Check Knative Serving Installation
----
kubectl get pods -n knative-serving
NAME                                READY   STATUS    RESTARTS   AGE
activator-764c84b867-hmdsq          1/1     Running   0          6h34m
autoscaler-6d78bfc6f-8v9d8          1/1     Running   0          6h34m
autoscaler-hpa-595fcbc958-r77tm     1/1     Running   0          6h34m
controller-7f9d9ffff7-9gws9         1/1     Running   0          6h34m
networking-istio-65cb5dc74d-dmfsh   1/1     Running   0          6h34m
webhook-8fb4f7f8-fzfvx              1/1     Running   0          6h34m
----

=== Check Knative Eventing Installation
----
kubectl get pods -n knative-eventing
NAME                                   READY   STATUS    RESTARTS   AGE
eventing-controller-666b79d867-kq8cc   1/1     Running   0          6h34m
eventing-webhook-5867c98d9b-hzctw      1/1     Running   0          6h34m
imc-controller-7c4f9945d7-s59xd        1/1     Running   0          6h34m
imc-dispatcher-7b55b86649-nsjm2        1/1     Running   0          6h34m
sources-controller-694f8df9c4-pss2w    1/1     Running   0          6h34m
----


=== Deploy Knative Serving Service

Knative Eventing targets a Kubernetes Service, in our case, we will use a Knative Serving Service as the target of the event - the Sink.

There are 3 options for Knative Serving Services to choose from:

* qeventinghello - Quarkus with JAX-RS API, offering both JVM and native 
* sbeventingce - Spring Boot with RestController API
* sbeventinghello - Spring Boot with Servlet API

The primary focus on the code is to receive the POST and to output the CloudEvent relevant information:

----
System.out.println("ce-id=" + http.getHeaders().get("ce-id"));
System.out.println("ce-source=" + http.getHeaders().get("ce-source"));
System.out.println("ce-specversion=" + http.getHeaders().get("ce-specversion"));
System.out.println("ce-time=" + http.getHeaders().get("ce-time"));
System.out.println("ce-type=" + http.getHeaders().get("ce-type"));
System.out.println("content-type=" + http.getHeaders().getContentType());
System.out.println("content-length=" + http.getHeaders().getContentLength());

System.out.println("POST:" + http.getBody());
----

CloudEvent to HTTP mapping information can be found here
https://github.com/cloudevents/spec/blob/master/http-protocol-binding.md#3-http-message-mapping


To deploy sbeventingce, first build its .jar and its container image:

----
cd sbeventingce
mvn clean compile package
eval $(minikube docker-env)
./dockerbuild.sh
cd ..
----

Deploy the Knative Serving Service into namespace myeventing:

----
kubectl create namespace myeventing
kubectl config set-context --current --namespace=myeventing

kubectl apply -f knativefiles/1_serving.yaml
----

Verify that eventinghello deployed

----
kubectl get ksvc
NAME            URL                                           LATESTCREATED      LATESTREADY        READY   REASON
eventinghello   http://eventinghello.myeventing.example.com   eventinghello-v1   eventinghello-v1   True
----

The default behavior of Knative Serving is that the very first deployment of a Knative Serving Service will automatically scale up to 1 and after about 90 seconds it will auto-scale down to zero.

Follow its logs

----
kubectl get pods

stern eventinghello -c user-container
----

Let eventinghello scale to zero pods before moving on.

=== Deploy CronJobSource
----
kubectl apply -f knativefiles/2_source2service.yaml

kubectl get cronjobsource
NAME                           READY   AGE
eventinghello-cronjob-source   True    10s
----

This produces a pod with a prefix of "cronjobsource-eventinghell"

----
kubectl get pods
NAME                                             READY   STATUS    RESTARTS   AGE
cronjobsource-eventinghell-6f6feb00-29af-11ea-   1/1     Running   0          16s
----

After about 2 minutes, it will send cause the eventinghello pod to scale up 

----
kubectl get pods -l serving.knative.dev/configuration=eventinghello
NAME                                           READY   STATUS    RESTARTS   AGE
eventinghello-v1-deployment-5c9989f9bd-245nh   2/2     Running   0          5s
----

And after about 60 seconds, eventinghello will auto-scale down to zero pods.  Then, upon the next 2 minute interval for the cronjob it will scale back up to 1.

=== Clean Up
----
kubectl delete namespace myeventing
----

== Kafka+Knative Eventing

In this section, we will deploy Kafka (via Strimzi), the Knative Kafka Source and have the messages flowing through the Kafka topic as the event that causes the scale-up of the sink Knative Serving Service called eventhinghello.

=== Deploy Kafka for Kubernetes (Strimzi) inside Minikube

https://strimzi.io/quickstarts/minikube/

----

kubectl create namespace kafka
kubectl config set-context --current --namespace=kafka

curl -L https://github.com/strimzi/strimzi-kafka-operator/releases/download/0.15.0/strimzi-cluster-operator-0.15.0.yaml \
  | sed 's/namespace: .*/namespace: kafka/' \
  | kubectl apply -f - -n kafka 

----

The result will be the single strimzi-cluster-operator
----
kubectl get pods
strimzi-cluster-operator-85f596bfc7-7dgds     1/1     Running   0          1m2s
----

=== Deploy a Kafka Cluster inside Minikube 

----
kubectl apply -f https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/0.15.0/examples/kafka/kafka-persistent-single.yaml -n kafka 
----

The result will be a single Zookeeper, single Kafka broker and the entity-operator

----
kubectl get pods 
NAME                                          READY   STATUS    RESTARTS   AGE
my-cluster-entity-operator-7d677bdf7b-jpws7   3/3     Running   0          85s
my-cluster-kafka-0                            2/2     Running   0          110s
my-cluster-zookeeper-0                        2/2     Running   0          2m22s
strimzi-cluster-operator-85f596bfc7-7dgds     1/1     Running   0          4m22s
----

=== Create Kafka Topic my-topic

----
cat <<EOF | kubectl apply -f -
apiVersion: kafka.strimzi.io/v1alpha1
kind: KafkaTopic
metadata:
  name: my-topic
  labels:
    strimzi.io/cluster: my-cluster
spec:
  partitions: 10
  replicas: 1
EOF
----

Check that the topic was created
----
kubectl get kafkatopics
kubectl describe kafkatopic my-topic
----

Create some test messages

Terminal 1 - Producer
----
../knative-tutorial/bin/kafka-producer.sh
>one
>two
>three
----

Terminal 2 - Consumer
----
../knative-tutorial/bin/kafka-consumer.sh
one
two
three
----

Ctrl-C to stop producer & consumer


=== Create the Knative Kafka Source Infrastructure

Create the Knative Kafka Source

----
kubectl apply -f https://github.com/knative/eventing-contrib/releases/download/v0.11.0/kafka-source.yaml
----

This step creates Knative Kafka Source in the knative-sources namespace as well as a CRD, ServiceAccount, ClusterRole, etc 

----
kubectl get pods -n knative-sources
NAME                         READY   STATUS    RESTARTS   AGE
kafka-controller-manager-0   1/1     Running   0          1m17s
----

Create the Knative Kafka Channel

----
curl -L "https://github.com/knative/eventing-contrib/releases/download/v0.11.0/kafka-channel.yaml" \
 | sed 's/REPLACE_WITH_CLUSTER_URL/my-cluster-kafka-bootstrap.kafka:9092/' \
 | kubectl apply --filename -  
----

note: "my-cluster-kafka-bootstrap.kafka:9092" comes from "kubectl get services -n kafka"

Look for 3 new pods in namespace knative-eventing with the prefix "kafka"

----
kubectl get pods -n knative-eventing
NAME                                   READY   STATUS    RESTARTS   AGE
eventing-controller-666b79d867-kq8cc   1/1     Running   0          64m
eventing-webhook-5867c98d9b-hzctw      1/1     Running   0          64m
imc-controller-7c4f9945d7-s59xd        1/1     Running   0          64m
imc-dispatcher-7b55b86649-nsjm2        1/1     Running   0          64m
kafka-ch-controller-7c596b6b55-fzxcx   1/1     Running   0          33s
kafka-ch-dispatcher-577958f994-4f2qs   1/1     Running   0          33s
kafka-webhook-74bbd99f5c-c84ls         1/1     Running   0          33s
sources-controller-694f8df9c4-pss2w    1/1     Running   0          64m  
----

and some new CRDs

----
kubectl get crds | grep kafkasources
kafkasources.sources.eventing.knative.dev            2019-12-28T14:53:14Z

kubectl get crds | grep kafkachannels
kafkachannels.messaging.knative.dev                  2019-12-28T15:00:22Z
----

=== Deploy Knative Serving Sink Service

First build the jar and the docker image
----
cd sbeventingce
mvn clean compile package
eval $(minikube docker-env)
./dockerbuild.sh
cd ..
----

Then deploy the Knative Serving Service
----
kubectl apply -f knativefiles/1_serving.yaml

# OR

kubectl apply -f knativefiles/1_serving_quay.yaml

kubectl get ksvc
----

Follow the logs

----
stern eventinghello -c user-container
----

=== Create KafkaSource for my-topic

----
cat <<EOF | kubectl apply -f -
apiVersion: sources.eventing.knative.dev/v1alpha1
kind: KafkaSource
metadata:
  name: mykafka-source
spec:
  consumerGroup: knative-group
  bootstrapServers: my-cluster-kafka-bootstrap:9092 
  topics: my-topic
  sink:
    apiVersion: serving.knative.dev/v1alpha1
    kind: Service
    name: eventinghello
EOF
----

This will result in a new pod prefixed with "mykafka-source". 
"eventinghello" will be around until it hits its scale-down time limit.

----
kubectl get pods
NAME                                           READY   STATUS    RESTARTS   AGE
eventinghello-v1-deployment-65c9b9c7df-7hdbl   2/2     Running   0          67s
my-cluster-entity-operator-7d677bdf7b-jpws7    3/3     Running   0          22m
my-cluster-kafka-0                             2/2     Running   0          22m
my-cluster-zookeeper-0                         2/2     Running   0          23m
mykafka-source-vxs2k-56548756cc-j7m7v          1/1     Running   0          11s
strimzi-cluster-operator-85f596bfc7-7dgds      1/1     Running   0          25m
----

Note: since we had some test messages of "one", "two" and "three" from earlier you should see the eventinghello service awaken to process those messages.  Since the "one", "two", and "three" were not entered as JSON, your http body will not be interpreted correctly.  Knative Eventing endpoints need JSON input.

Wait the 90+ seconds for eventinghello to scale down before moving on.

=== Publish some messages

Note: Knative Eventing messages needs to be JSON formatted

----
../knative-tutorial/bin/kafka-producer.sh

{"hello":"world"}

{"hola":"mundo"}

{"bonjour":"le monde"}

{"hey": "duniya"}

----

Ctrl-C to terminate producer

=== Produce a bunch of messages

The Knative Serving Sink Service was defined with the following annotation

----
autoscaling.knative.dev/target: "1"
----

This means a concurrency factor of one, if you are able to push in a lot of Kafka message rapidly, you will see more than one eventinghello pod scaled up to handle the load.

Build & Deploy the simple Kafka Spammer application to push messages faster

----
cd kafkaspammer
./1_jvmbuild.sh
eval $(minikube docker-env)
./2_dockerbuild_jvm.sh
./3_deploy.sh
----

Now you just need to hit the right endpoint on the Kafka Spammer application to push in 3 messages.

----
URL=$(minikube ip):$(kubectl get service/kafka-spammer -o jsonpath="{.spec.ports[*].nodePort}" -n kafka)
curl $URL/3
----

OR use a pre-built version
----
kubectl -n kafka run kafka-spammer --image=quay.io/burrsutter/kafkaspammer:1.0.2 

kubectl exec -i -t $(kubectl get pod -l "run=kafka-spammer" -o jsonpath='{.items[0].metadata.name}') -- /bin/sh

curl localhost:8080/3

----

close out the spammer 

----
exit 

kubectl delete deployment kafka-spammer
----


You should see about 3 eventinghello pods springing to life
----
kubectl get pods  
NAME                                           READY   STATUS    RESTARTS   AGE
eventinghello-v1-deployment-65c9b9c7df-8rwqc   1/2     Running   0          6s
eventinghello-v1-deployment-65c9b9c7df-q7pcf   1/2     Running   0          4s
eventinghello-v1-deployment-65c9b9c7df-zht2t   1/2     Running   0          6s
kafka-spammer-77ccd4f9c6-sx5j4                 1/1     Running   0          26s
my-cluster-entity-operator-7d677bdf7b-jpws7    3/3     Running   0          27m
my-cluster-kafka-0                             2/2     Running   0          27m
my-cluster-zookeeper-0                         2/2     Running   0          28m
mykafka-source-vxs2k-56548756cc-j7m7v          1/1     Running   0          5m12s
strimzi-cluster-operator-85f596bfc7-7dgds      1/1     Running   0          30m
----

After about 60 seconds and no new messages showing up in my-topic, the 3 eventinghello pods will terminate, scale-down to zero

Try 10 messages
----
URL=$(minikube ip):$(kubectl get service/kafka-spammer -o jsonpath="{.spec.ports[*].nodePort}" -n kafka)
curl $URL/10
----

Note: these messages are NOT being evenly distributed across the various eventinghello pods, the first one up starts consuming them immediately.

image::sending_10.png[Sending 10 messages]

A little video that shows the scaling in action

video::scale_up_down_10.mp4[width=950]

== Knative Eventing: Channels & Subscriptions

If you want more than one Sink use Channels and Subscriptions to decouple the producers & consumers of events

=== Channels

5 Step Process

1) Create a Namespace

2) Create Channel

3) Create Source to Channel

4) Create Sink Service

5) Create Subscription of Channel to Sink Service

=== 1. Create Namespace
----
kubectl create namespace myeventing
kubectl config set-context --current --namespace=myeventing
----

=== 2. Create Channel

----
cat <<EOF | kubectl apply -f -
apiVersion: messaging.knative.dev/v1alpha1
kind: InMemoryChannel
metadata:
  name: eventinghello-ch
EOF
----

----
kubectl get inmemorychannel
kubectl describe inmemorychannel eventinghello-ch
----

=== 3. Create Source that will send to the Channel

----
cat <<EOF | kubectl apply -f -
apiVersion: sources.eventing.knative.dev/v1alpha1
kind: CronJobSource
metadata:
  name: my-cjs
spec:
  schedule: "*/2 * * * *"
  data: '{"message": "From CronJob Source"}'
  sink:
    apiVersion: messaging.knative.dev/v1alpha1
    kind: InMemoryChannel
    name: eventinghello-ch
EOF
----


----
kubectl get cronjobsource
kubectl describe cronjobsource my-cjs
----


=== 4. Create Sink Services

----
cat <<EOF | kubectl apply -f -
apiVersion: serving.knative.dev/v1alpha1
kind: Service
metadata:
  name: eventinghelloa
spec:
  template:
    metadata:
      name: eventinghelloa-1
      annotations:
        # disable istio-proxy injection
        sidecar.istio.io/inject: "false"
        autoscaling.knative.dev/target: "1"
    spec:
      containers:
      - image: quay.io/burrsutter/eventinghello:0.0.1
EOF
----

----
cat <<EOF | kubectl apply -f -
apiVersion: serving.knative.dev/v1alpha1
kind: Service
metadata:
  name: eventinghellob
spec:
  template:
    metadata:
      name: eventinghellob-2
      annotations:
        # disable istio-proxy injection
        sidecar.istio.io/inject: "false"
        autoscaling.knative.dev/target: "1"
    spec:
      containers:
      - image: quay.io/burrsutter/eventinghello:0.0.1
EOF
----

=== 5. Create Subscriptions to Channel 

----
cat <<EOF | kubectl apply -f -
apiVersion: messaging.knative.dev/v1alpha1
kind: Subscription
metadata:
  name: eventinghelloa-sub
spec:
  channel:
    apiVersion: messaging.knative.dev/v1alpha1
    kind: InMemoryChannel
    name: eventinghello-ch
  subscriber:
    ref:
      apiVersion: serving.knative.dev/v1alpha1
      kind: Service
      name: eventinghelloa
EOF
----

----
cat <<EOF | kubectl apply -f -
apiVersion: messaging.knative.dev/v1alpha1
kind: Subscription
metadata:
  name: eventinghellob-sub
spec:
  channel:
    apiVersion: messaging.knative.dev/v1alpha1
    kind: InMemoryChannel
    name: eventinghello-ch
  subscriber:
    ref:
      apiVersion: serving.knative.dev/v1alpha1
      kind: Service
      name: eventinghellob
EOF
----

Wait the 2 minutes for the cronjobsource and see eventinghelloa and b start to run

----
kubectl get pods
NAME                                                              READY   STATUS    RESTARTS   AGE
cronjobsource-my-cjs-93544f14-2bf9-11ea-83c7-08002737670c-6br6x   1/1     Running   0          2m15s
eventinghelloa-1-deployment-d86bf4847-hvbk6                       2/2     Running   0          5s
eventinghellob-2-deployment-5c986c7586-4clpb                      2/2     Running   0          5s
----

Wait approximately 60 seconds for eventinghelloa and b to auto-scaled down to zero

----
kubectl get pods
NAME                                                              READY   STATUS        RESTARTS   AGE
cronjobsource-my-cjs-93544f14-2bf9-11ea-83c7-08002737670c-6br6x   1/1     Running       0          7m15s
eventinghelloa-1-deployment-d86bf4847-hvbk6                       2/2     Terminating   0          65s
eventinghellob-2-deployment-5c986c7586-4clpb                      2/2     Terminating   0          65s
----

== Brokers & Triggers

1. Create the namespace and inject the broker into your namespace
2. Create the consumers/receivers
3. Create triggers
4. Push some messages

=== 1. Create the namespace and inject the broker into your namespace

----
kubectl create namespace myeventing
kubectl config set-context --current --namespace=myeventing
kubectl label namespace myeventing knative-eventing-injection=enabled
----

----
kubectl --namespace myeventing get Broker 
----

----
kubectl get pods
NAME                                           READY   STATUS        RESTARTS   AGE
default-broker-filter-c6654bccf-qb272          1/1     Running       0          62s
default-broker-ingress-7479966dc7-99xvm        1/1     Running       0          62s
----

=== 2. Create the consumers/receivers

----
cat <<EOF | kubectl apply -f -
apiVersion: serving.knative.dev/v1alpha1
kind: Service
metadata:
  name: eventingbonjour
spec:
  template:
    metadata:
      name: eventingbonjour-1
      annotations:
        # disable istio-proxy injection
        sidecar.istio.io/inject: "false"
        autoscaling.knative.dev/target: "1"
    spec:
      containers:
      - image: quay.io/burrsutter/eventinghello:0.0.1
EOF
----

----
cat <<EOF | kubectl apply -f -
apiVersion: serving.knative.dev/v1alpha1
kind: Service
metadata:
  name: eventingaloha
spec:
  template:
    metadata:
      name: eventingaloha-1
      annotations:
        # disable istio-proxy injection
        sidecar.istio.io/inject: "false"
        autoscaling.knative.dev/target: "1"
    spec:
      containers:
      - image: quay.io/burrsutter/eventinghello:0.0.1
EOF
----

----
kubectl get ksvc
NAME              URL                                             LATESTCREATED       LATESTREADY         READY   REASON
eventingaloha     http://eventingaloha.myeventing.example.com     eventingaloha-1     eventingaloha-1     True
eventingbonjour   http://eventingbonjour.myeventing.example.com   eventingbonjour-1   eventingbonjour-1   True
----

=== 3. Create triggers
----
kubectl apply --filename - << EOF
apiVersion: eventing.knative.dev/v1alpha1
kind: Trigger
metadata:
  name: hellobonjour
spec:
  filter:
    attributes:
      type: greeting
  subscriber:
    ref:
     apiVersion: serving.knative.dev/v1alpha1
     kind: Service
     name: eventingbonjour
EOF
----

----
kubectl apply --filename - << EOF
apiVersion: eventing.knative.dev/v1alpha1
kind: Trigger
metadata:
  name: helloaloha
spec:
  filter:
    attributes:
      type: greeting
  subscriber:
    ref:
     apiVersion: serving.knative.dev/v1alpha1
     kind: Service
     name: eventingaloha
EOF
----

----
kubectl get triggers
NAME           READY   REASON   BROKER    SUBSCRIBER_URI                                        AGE
helloaloha     True             default   http://eventingaloha.myeventing.svc.cluster.local     24s
hellobonjour   True             default   http://eventingbonjour.myeventing.svc.cluster.local   48s
----

Get their subscriberURIs
----
kubectl get trigger hellobonjour -o jsonpath='{.status.subscriberURI}'
http://eventingbonjour.myeventing.svc.cluster.local
----

----
kubectl get trigger helloaloha -o jsonpath='{.status.subscriberURI}'
http://eventingaloha.myeventing.svc.cluster.local
----

You will need those URIs in a below

=== 4. Push some messages

Start stream the logs for the event consumers

----
stern eventing -c user-container
----

Create a pod for using Curl
----
kubectl apply --filename - << EOF
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: curler
  name: curler
spec:
  containers: 
  - name: curler 
    image: fedora:29
    tty: true
EOF
----

Exec into the pod
----
kubectl exec -i -t $(kubectl get pod -l "run=curler" -o jsonpath='{.items[0].metadata.name}') -- /bin/bash
----

Curl the subcriberURI for eventingbonjour

----
curl -v "http://eventingbonjour.myeventing.svc.cluster.local" \
-X POST \
-H "Ce-Id: say-hello" \
-H "Ce-Specversion: 0.3" \
-H "Ce-Type: bonjour" \
-H "Ce-Source: mycurl" \
-H "Content-Type: application/json" \
-d '{"key":"from a curl"}'
----

Curl the subcriberURI for eventingaloha

----
curl -v "http://eventingaloha.myeventing.svc.cluster.local" \
-X POST \
-H "Ce-Id: say-hello" \
-H "Ce-Specversion: 0.3" \
-H "Ce-Type: aloha" \
-H "Ce-Source: mycurl" \
-H "Content-Type: application/json" \
-d '{"key":"from a curl"}'
----

Curl the subcriberURI for broker

----
kubectl get broker default -o jsonpath='{.status.address.url}'
http://default-broker.myeventing.svc.cluster.local
----

Note: Ce-Type: greeting

----
curl -v "http://default-broker.myeventing.svc.cluster.local" \
-X POST \
-H "Ce-Id: say-hello" \
-H "Ce-Specversion: 0.3" \
-H "Ce-Type: greeting" \
-H "Ce-Source: mycurl" \
-H "Content-Type: application/json" \
-d '{"key":"from a curl"}'
----

Both eventingbonjour and eventingaloha will come to life
----
eventingaloha-1-deployment-6fd4689467-xfsgv     2/2     Running       0          5s
eventingbonjour-1-deployment-677bbf6598-8mfd9   2/2     Running       0          5s
----


=== Default Channel

In addition to using Kafka as an event Source, you can replace the default in-memory channel of Knative Eventing with a Kafka broker as well.

First inspect the default configuration for Knative Eventing

----
kubectl get cm default-ch-webhook -n knative-eventing -o yaml --export

apiVersion: v1
data:
  default-ch-config: |
    clusterDefault:
      apiVersion: messaging.knative.dev/v1alpha1
      kind: InMemoryChannel
    namespaceDefaults:
      some-namespace:
        apiVersion: messaging.knative.dev/v1alpha1
        kind: InMemoryChannel
kind: ConfigMap
metadata:
  annotations:
  creationTimestamp: null
  name: default-ch-webhook
  selfLink: /api/v1/namespaces/knative-eventing/configmaps/default-ch-webhook
----

Now, apply a new configuration that adds the "kafka" namespace (or whichever namespace you are working with)

----
cat <<-EOF | kubectl apply -f -
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: default-ch-webhook
  namespace: knative-eventing
data:
  default-ch-config: |
    clusterDefault:
      apiVersion: messaging.knative.dev/v1alpha1
      kind: InMemoryChannel
    namespaceDefaults:
      kafka:
        apiVersion: messaging.knative.dev/v1alpha1
        kind: KafkaChannel
        spec:
          numPartitions: 1
          replicationFactor: 1
EOF
----

kubectl delete pod -l app=webhook -n knative-serving

kubectl exec -it -n kafka -c kafka my-cluster-kafka-0 /bin/bash

cd bin

ls

./kafka-topics.sh --zookeeper localhost:2181 --list

./kafka-topics.sh --zookeeper localhost:2181 --describe --topic my-topic

